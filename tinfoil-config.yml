shim-version: v0.1.1@sha256:fcd0ba0ea3f0ca26d92595ea90f28308cfba819e3243925892e6c8f21eb1397d
cvm-version: 0.4.1
cpus: 32
memory: 524288
gpus: full
vllm: false

models:
  - name: "llama3-3-70b-fp8"
    repo: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic@984c96b73bcf6a675945bac6382b9ed551e5d42b"
    mpk: "0d0be05062e4d3fadc17176cffd1ef6b518a02b0410bd897723f423074ec6cb5_72687374336_99f5f660-3ee0-5626-9772-2f082314e763"
  
containers:
  - name: "llama3-3-70b-fp8"
    image: ""
    args: [
      "--runtime", "nvidia",
      "--gpus", "all",
      "--ipc", "host",
      "vllm/vllm-openai:v0.9.2",
      "--model", "/tinfoil/mpk/mpk-0d0be05062e4d3fadc17176cffd1ef6b518a02b0410bd897723f423074ec6cb5",
      "--tensor-parallel-size", "2",
      "--max-model-len", "32768",
      "--enforce-eager",
      "--served-model-name", "llama3-3-70b-fp8",
      "--port", "8001"
    ]

shim:
  listen-port: 443
  upstream-port: 8001
  publish-attestation: false
  tls-challenge: dns  
  paths:
    - /v1/chat/completions
    - /metrics
  origins:
    - https://tinfoil.sh
    - https://chat.tinfoil.sh
    - http://localhost:3000
